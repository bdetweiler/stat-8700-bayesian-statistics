---
title: "STAT 8700 Homework 10"
author: "Brian Detweiler"
date: "Tuesday, November 22th, 2016"
header-includes:
  - \usepackage{color}
  - \usepackage{xcolor}
  - \usepackage{soul}
  - \usepackage{hyperref}
  
output: pdf_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=TRUE, warning=FALSE, message=FALSE)
```

```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(rstan)
library(rjags)
source("DBDA2Eprograms/DBDA2E-utilities.R")
```

# 1. Consider the data presented in Table 7.3

```{r}
blue.earth <- c(5.0, 13.0, 7.2, 6.8, 12.8,  5.8,  9.5, 6.0, 3.8,  14.3, 1.8,  6.9, 4.7, 9.5)
blue.earth.level <- c(0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0)
clay <- c(0.9, 12.9, 2.6, 3.5, 26.6,  1.5, 13.0, 8.8, 19.5,  2.5, 9.0, 13.1, 3.6, 6.9)
clay.level <- c(1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1)
goodhue <-    c(14.3, 6.9, 7.6, 9.8,  2.6, 43.5,  4.9, 3.5,  4.8,  5.6, 3.5,  3.9, 6.7)
goodhue.level <- c(0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0)
radon.df <- cbind(blue.earth, blue.earth.level, clay, clay.level, goodhue, goodhue.level)
radon.df

basements.blue.earth <- ((1 + blue.earth.level) %% 2)
basements.clay <- ((1 + clay.level) %% 2)
basements.goodhue <- ((1 + goodhue.level) %% 2)

basement.data <- data.frame(y=sum(basements.blue.earth), N=length(basements.blue.earth))
basement.data <- rbind(basement.data, data.frame(y=sum(basements.clay), N=length(basements.clay)))
basement.data <- rbind(basement.data, data.frame(y=sum(basements.goodhue), N=length(basements.goodhue)))
basement.data
```

## (a) Let $\theta_1$, $\theta_2$, and $\theta_3$ represent the proportion of houses that have basements in Blue Earth, Clay, and Goodhue counties respectively. Fit a hierarchical model to the data, and use it to obtain posterior summaries for $\theta_1$, $\theta_2$, and $\theta_3$.

```{r}

# Log Posterior (u, v space)
log.post2 <- function(u, v) {
  
  basements <- basement.data$y
  houses <- basement.data$N
  alpha <- exp(u + v) / (1 + exp(u))
  beta <- exp(v) / (1 + exp(u))
  ldens <- 0
  
  # Loop over each of the 71 experiments
  for(i in 1:length(houses)) {
    
    # There is no gamma function in R - have to use Log Gamma, so the density is logged
    # This is why it's addative rather than multiplicative
    # basements[i] is the same as y_i
    # houses[i] is the same as n_i
    ldens <- (ldens
             + (lgamma(alpha + beta) + lgamma(alpha + basements[i]) + lgamma(beta + houses[i] - basements[i]))
             - (lgamma(alpha) + lgamma(beta) + lgamma(alpha + beta + houses[i])))
  }

  # Return the final posterior density, which is still in logged form
  ldens - 5 / 2 * log(alpha + beta) + log(alpha) + log(beta)
}

# Just defines the size of each contour: 0.05, 0.15, 0.25, ..., 0.95
contours <- seq(0.05, 0.95, 0.1)

# Do the same steps as above, but refine the grid space
# Also, I changed the length to 200, because 2001 was slowing my computer to a crawl
u2 <- seq(-4, 4, length = 200)
v2 <- seq(-5, 13, length = 200)
logdens2 <- outer(u2, v2, log.post2)
# dens2 is a 200x200 matrix of probabilities for the u2, v2 values of alpha and beta
# For instance, u2[1] = -2.3, and v2[1] = 1. dens2[1, 1] = 1.978023e-14
# This is equivalent to saying p(alpha = -2.3, beta = 1) = 1.978023e-14
dens2 <- exp(logdens2 - max(logdens2))

fileName <- "Assignment_10_1_a"

modelString ="
model {

  for (j in 1:count) {
    y[j] ~ dbin(theta[j], N[j])
    theta[j] ~ dbeta(alpha, beta)
  }




  lnx <- log(alpha / beta)
  lny <- log(alpha + beta)

  alpha <- u / pow(v, 2)
  beta <- (1 - u) / pow(v, 2)

  u ~ dunif(-2, 4)
  v ~ dunif(-5, 13)
  #u ~ dunif(0.09, 0.22)
  #v ~ dunif(0.08, 0.61)
}
"

writeLines(modelString, con=fileName)

basementsModel = jags.model(file=fileName, 
                             data=list(y=basement.data$y,
                                       N=basement.data$N, 
                                       count=length(basement.data$N)),
                             n.chains=4)

update(basementsModel, n.iter=10000)


basementsSamples <- coda.samples(basementsModel, n.iter=200000, variable.names=c("alpha", "beta", "theta", "y", "lnx", "lny"), thin=20)

basementsSamples.M <- as.matrix(basementsSamples)

summary(basementsSamples.M)

contour(u2, v2, dens2, levels = contours, drawlabels = FALSE, xlim=c(-2, 4), ylim=c(-5, 13))
points(basementsSamples.M[,"lnx"], basementsSamples.M[,"lny"], col="red", pch=".", xlim=c(-2, 4), ylim=c(-5, 13))

```


## (b) Fit a linear regression to the natural log of the radon measurements, with indicator variables for the three counties and for weather a measurement was recorded on the first floor or basement, do not include an intercept term. Present posterior summaries for parameters and summarize your posterior inferences in non-technical terms (int, for each parameter $\beta$, what does $e^{\beta}$ represent?)

## (c) Suppose another house is sampled at random from Clay County, simulate values from the posterior predictive distribution for its radon measurements anf give an 95\% predictive interval. Express the interval of the original unlogged scale. (Hint: You must consider whether or not the randomly chosen house has a basement)

\begin{flushright}
  $\blacksquare$
\end{flushright}

\pagebreak

# 2. The file \texttt{drinks.txt} contains the amount of time needed by a company employee to refill an automatic vending machine. For each refill, the number of cases of product and the distance walked (in feet) is also recorded.

```{r}
drinks <- read.table('drinks.txt', header=TRUE)
```

## (a) Fit a linear regression model for the time taken, with number of cases and distance walked as explanatory variables (include an intercept term).

## (b) In Classical Statistics, one way the quality of a regression model can be analyzed is by calculating something called the $AdjustedR^2$ value, it is basically the pro-portion of variation in the response variable that is explained by the explanatory variables, adjusted for the number of variables. Obviously, the closer this number is to 1, the better. The Bayesian equivalent R2B is defined as

$$
\begin{split}
  R_B^2 &= 1 - \frac{\sigma^2}{S_Y^2}
\end{split}
$$

## where $\sigma^2$ is the variance of the regression model and $s_Y^2$ is the sample variance of the response variable data. Note that since in the Bayesian framework $\sigma^2$ is a random variable, so is $R_B^2$. Obtain a 95\% credible interval for $R_B^2$. Does it seem like the model is a good for the data?

## (c) Obtain a 95\% predictive interval for how long it would take to restock the vending machine if the number of cases and distance were at their average (mean) values.

\begin{flushright}
  $\blacksquare$
\end{flushright}

\pagebreak

# 3. Revisit the data in Question 1. Fit a Two-Way ANOVA to the natural log of the radon measurements. For each county in separately, construct a 95\% credible interval for the difference between the average (unlogged) radon measurement in houses with basements and in houses without basements. Test the hypothesis that the average radon measurement in houses with basements is greater than in houses without basements.


\begin{flushright}
  $\blacksquare$
\end{flushright}

\pagebreak