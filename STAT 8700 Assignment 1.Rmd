---
title: "STAT 8700 Homework 1"
author: "Brian Detweiler"
date: "Friday, September 2, 2016"
output: pdf_document
---

# 1. Read Chapter 1.

Done.

# 2. Consider an urn containing 9 balls, which can be either red or green.  Let $X$ be the number of red balls in the urn and before observing any balls we will assume that all possible values of $X$ from 0 to 9 are equally likely. Suppose we plan to draw 3 balls from the urn, and let $Y_i = 1$ if the $i^{th}$ ball is red, and $Y_i = 0$ if the $i^{th}$ ball is green for $i = 1, 2, 3$. When we draw the 3 balls, we observe $Y_1 = 1, Y_2 = 1$, and $Y_3 = 0$. As per our examples in class, construct a table with columns $X$, Prior, Likelihood, Likelihood x Prior, and Posterior to obtain the Posterior Distribution of $X$.

Our likelihood is given by:
$$
  \begin{split}
    P(Y_1 = 1, Y_2 = 1, Y_3 = 0 | X = x)
  \end{split}
$$

We want to find the posterior distribution, such that
$$
  \begin{split}
    P(X = x | Y_1 = 1, Y_2 = 1, Y_3 = 0) &= \frac{P(Y_1 = 1, Y_2 = 1, Y_3 = 0 | X = x) \cdot P(X = x)}{P(Y_1 = 1, Y_2 = 1, Y_3 = 0)}\\
  \end{split}
$$

```{r}
hypergeo <- function(numberRedDrawn, numberRed, numberWhite, totalDrawn) {
  return(dhyper(numberRedDrawn, numberRed, numberWhite, totalDrawn))
}

# For Y_1 = 1
totalInUrn <- 9
minNumberOfRedInUrn <- 2
# X = x for all possible x
numberOfRedInUrn <- c(minNumberOfRedInUrn:totalInUrn)
numberOfRedDrawn <- 1
numberOfWhiteInUrn <- totalInUrn - numberOfRedInUrn
totalDrawn <- 1

Y1 <- hypergeo(numberOfRedDrawn, numberOfRedInUrn, numberOfWhiteInUrn, totalDrawn)
Y1 <- append(c(0, 0), Y1)
Y1

# For Y_2 = 1, having already drawn Y_1 = 1
totalInUrn <- 8
minNumberOfRedInUrn <- 1
# X = x for all possible x
numberOfRedInUrn <- c(minNumberOfRedInUrn:totalInUrn)
numberOfRedDrawn <- 1
numberOfWhiteInUrn <- totalInUrn - numberOfRedInUrn
totalDrawn <- 1

Y2 <- hypergeo(numberOfRedDrawn, numberOfRedInUrn, numberOfWhiteInUrn, totalDrawn)
Y2 <- append(c(0, 0), Y2)
Y2

# For Y_3 = 0, having already drawn Y_1 = 1 and Y_2 = 2
totalInUrn <- 7
minNumberOfRedInUrn <- 0
# X = x for all possible x
numberOfRedInUrn <- c(minNumberOfRedInUrn:totalInUrn)
numberOfRedDrawn <- 0
numberOfWhiteInUrn <- totalInUrn - numberOfRedInUrn
totalDrawn <- 1

Y3 <- hypergeo(numberOfRedDrawn, numberOfRedInUrn, numberOfWhiteInUrn, totalDrawn)
Y3 <- append(c(0, 0), Y3)
Y3

conditionalLikelihood <- Y1 * Y2 * Y3
conditionalLikelihood
sum(conditionalLikelihood)

prior <- 1/10

priorTimesLikelihood <- conditionalLikelihood * prior
priorTimesLikelihood

constant <- sum(priorTimesLikelihood)
constant

posterior <- priorTimesLikelihood / constant
plot(posterior)
posterior
sum(posterior)

```

\begin{center}
\begin{tabular}{ c | c | c | c | c | }
   $X$          & Prior       & Likelihood &  Prior $\times$ Likelihood & Prior $\times$ Likelihood / constant = Posterior  \\
    [1ex] 
   \hline
   0 & $\frac{1}{10}$  & 0 & 0 & 0 \\
    [1ex] 
   \hline
   1 & $\frac{1}{10}$  & 0 & 0 & 0 \\
    [1ex] 
   \hline
   2 & $\frac{1}{10}$  & 0.02777778 & 0.002777778 & 0.03333333 \\
    [1ex] 
   \hline
   3 & $\frac{1}{10}$  & 0.07142857 & 0.007142857 & 0.08571429 \\
    [1ex] 
   \hline
   4 & $\frac{1}{10}$  & 0.11904762 & 0.011904762 & 0.14285714 \\
    [1ex] 
   \hline
   5 & $\frac{1}{10}$  & 0.15873016 & 0.015873016 & 0.19047619 \\
    [1ex] 
   \hline
   6 & $\frac{1}{10}$  & 0.17857143 & 0.017857143 & 0.21428571 \\
    [1ex] 
   \hline
   7 & $\frac{1}{10}$  & 0.16666667 & 0.016666667 & 0.20000000 \\
    [1ex] 
   \hline
   8 & $\frac{1}{10}$  & 0.11111111 & 0.011111111 & 0.13333333 \\
    [1ex]
   \hline
   9 & $\frac{1}{10}$  & 0 & 0 & 0 \\
    [1ex] 
  \hline\hline
Total: & 1 & 0.8333333 & constant = 0.08333333 = $\frac{1}{12}$ & 1\\
\end{tabular}
\end{center}

\begin{flushright}
  $\blacksquare$
\end{flushright}

\pagebreak

# 3. Let $Y_1$ be the number of successes in $n = 10$ independent trials where each trial results in a success or failure, and $\theta$, the probability of success in each trial is the same for each trial.  Suppose we believe there are 4 possible values of $\theta$, $\frac{1}{5}, \frac{2}{5}, \frac{3}{5}, \frac{4}{5}$, which we view as equally likely. Now suppose we observe $Y_1 = 7$, use a table similar to the previous question to find the posterior distribution of $\theta$.

We want to find the posterior distribution 
$$
\begin{split}
  P(\theta | Y_1 = 7) &= \frac{P(Y_1 = 7 | \theta) \cdot P(\theta)}{P(Y_1 = 7)}\\
\end{split}
$$

Since we have no idea what $P(\theta)$ might be, we'll start with a uniform prior distribution, $U(0, 1)$. 

The likelihood is given by the binomial distribution,

$$
\begin{split}
  P(Y_1 = 7 | \theta) &= \binom{10}{7} \theta^7 (1 - \theta)^3\\
\end{split}
$$

```{r}
binomProb <- function(n, y, theta) {
  return(choose(n, y) * theta^y * (1 - theta)^(n - y))
}

prior <- c(1/4, 1/4, 1/4, 1/4)

n <- 10
successes <- 7

theta1 <- binomProb(n, successes, 1/5)
theta2 <- binomProb(n, successes, 2/5)
theta3 <- binomProb(n, successes, 3/5)
theta4 <- binomProb(n, successes, 4/5)

likelihood <- c(theta1, theta2, theta3, theta4)
likelihood

likelihoodTotal <- sum(likelihood)
likelihoodTotal

likelihoodTimesPrior <- likelihood * prior
likelihoodTimesPrior

constant <- sum(likelihoodTimesPrior)
constant

posterior <- likelihoodTimesPrior / constant
posterior

sum(posterior)

```

\begin{center}
\begin{tabular}{ c | c | c | c | c | }
   $\theta$      & Prior & Likelihood &  Prior $\times$ Likelihood & Prior $\times$ Likelihood / constant = Posterior  \\
    [1ex] 
   \hline
   $\frac{1}{5}$ & $\frac{1}{4}$  & 0.000786432 & 0.000196608 & 0.00171123 \\
    [1ex] 
   \hline
   $\frac{2}{5}$ & $\frac{1}{4}$  & 0.04246733  & 0.010616832 & 0.09240642 \\
    [1ex] 
   \hline
   $\frac{3}{5}$ & $\frac{1}{4}$  & 0.2149908   & 0.053747712 & 0.46780749 \\
    [1ex] 
   \hline
   $\frac{4}{5}$ & $\frac{1}{4}$  & 0.2013266   & 0.050331648 & 0.43807487 \\
    [1ex] 
  \hline\hline
Total: & 1 & 0.4595712 & constant = 0.1148928 & 1\\
\end{tabular}
\end{center}

\begin{flushright}
  $\blacksquare$
\end{flushright}

\pagebreak


# 4. Following on from the previous question, suppose we observe another 5 independent trials and $Y_2 = 2$ successes are observed in those 5 trials. Use the posterior distribution for $\theta$ from the previous question as the new prior distribution of $\theta$ and use a table to find the new posterior distribution of $\theta$ based on the added trials.


We want to find the posterior distribution 
$$
\begin{split}
  P(\theta | Y_2 = 2) &= \frac{P(Y_2 = 2 | \theta) \cdot P(\theta)}{P(Y_2 = 2)}\\
\end{split}
$$

Since we have some information about $P(\theta)$, we'll use the posterior, $P(\theta | Y_2 = 2)$ for our prior, $P(\theta)$.

The likelihood is still given by the binomial distribution,

$$
\begin{split}
  P(Y_2 = 2 | \theta) &= \binom{5}{2} \theta^2 (1 - \theta)^3\\
\end{split}
$$

```{r}
binomProb <- function(n, y, theta) {
  return(choose(n, y) * theta^y * (1 - theta)^(n - y))
}

prior <- posterior
prior

n <- 5
successes <- 2

theta1 <- binomProb(n, successes, 1/5)
theta2 <- binomProb(n, successes, 2/5)
theta3 <- binomProb(n, successes, 3/5)
theta4 <- binomProb(n, successes, 4/5)

likelihood <- c(theta1, theta2, theta3, theta4)
likelihood

likelihoodTotal <- sum(likelihood)
likelihoodTotal

likelihoodTimesPrior <- likelihood * prior
likelihoodTimesPrior

constant <- sum(likelihoodTimesPrior)
constant

posterior <- likelihoodTimesPrior / constant
posterior

sum(posterior)

```

\begin{center}
\begin{tabular}{ c | c | c | c | c | }
   $\theta$      & Prior & Likelihood &  Prior $\times$ Likelihood & Prior $\times$ Likelihood / constant = Posterior  \\
    [1ex] 
   \hline
   $\frac{1}{5}$ & 0.00171123 & 0.2048 & 0.0003504599 &  0.002156698  \\
    [1ex] 
   \hline
   $\frac{2}{5}$ & 0.09240642 & 0.3456 & 0.0319356578 & 0.196529065  \\
    [1ex] 
   \hline
   $\frac{3}{5}$ & 0.46780749 & 0.2304 & 0.1077828449 & 0.663285594  \\
    [1ex] 
   \hline
   $\frac{4}{5}$ &  0.43807487 & 0.0512 & 0.0224294332 & 0.138028644 \\
    [1ex] 
  \hline\hline
Total: & 1 & 0.4336128 & constant = 0.1624984 & 1\\
\end{tabular}
\end{center}

\begin{flushright}
  $\blacksquare$
\end{flushright}

\pagebreak



# 5. Suppose we combine all 15 trials from questions 3 and 4 together and think of them as a single set of data in which we observed 9 successes. Starting with our initial uniform prior, use a table to  nd the posterior distribution of $\theta$. Compare your answer to your answer at the end of question 4.


We want to find the posterior distribution 
$$
\begin{split}
  P(\theta | Y_3 = 9) &= \frac{P(Y_3 = 9 | \theta) \cdot P(\theta)}{P(Y_3 = 9)}\\
\end{split}
$$

As stated, we'll start with a uniform prior distribution, $U(0, 1)$.

The likelihood is given by the binomial distribution,

$$
\begin{split}
  P(Y_3 = 9 | \theta) &= \binom{15}{9} \theta^9 (1 - \theta)^6\\
\end{split}
$$

```{r}
binomProb <- function(n, y, theta) {
  return(choose(n, y) * theta^y * (1 - theta)^(n - y))
}

prior <- c(1/4, 1/4, 1/4, 1/4)

n <- 15
successes <- 9

theta1 <- binomProb(n, successes, 1/5)
theta2 <- binomProb(n, successes, 2/5)
theta3 <- binomProb(n, successes, 3/5)
theta4 <- binomProb(n, successes, 4/5)

likelihood <- c(theta1, theta2, theta3, theta4)
likelihood

likelihoodTotal <- sum(likelihood)
likelihoodTotal

likelihoodTimesPrior <- likelihood * prior
likelihoodTimesPrior

constant <- sum(likelihoodTimesPrior)
constant

posterior <- likelihoodTimesPrior / constant
posterior

sum(posterior)

```

\begin{center}
\begin{tabular}{ c | c | c | c | c | }
   $\theta$      & Prior & Likelihood &  Prior $\times$ Likelihood & Prior $\times$ Likelihood / constant = Posterior  \\
    [1ex] 
   \hline
   $\frac{1}{5}$ & $\frac{1}{4}$  & 0.0006717597 & 0.0001679399 & 0.002156698 \\
    [1ex] 
   \hline
   $\frac{2}{5}$ & $\frac{1}{4}$  & 0.0612141053 & 0.0153035263 & 0.196529065 \\
    [1ex] 
   \hline
   $\frac{3}{5}$ & $\frac{1}{4}$  & 0.2065976053 & 0.0516494013 & 0.663285594 \\
    [1ex] 
   \hline
   $\frac{4}{5}$ & $\frac{1}{4}$  & 0.0429926226 & 0.0107481557 & 0.138028644 \\
    [1ex] 
  \hline\hline
Total: & 1 & 0.3114761 & constant = 0.07786902 & 1\\
\end{tabular}
\end{center}

Interestingly, we get the exact same posterior as in **4**.

\begin{flushright}
  $\blacksquare$
\end{flushright}


\pagebreak



# 6. In R, install a package called Bolstad. This package includes a function called binodp, which stands for "Binomial Data, Discrete Prior", exactly like the situation described in questions 3, 4, and 5.  The function requires 4 inputs, the number of successes, the number of trials, a vector containing the possible values of $\theta$, and a vector containing the corresponding prior probabilities. Note: In R, a vector is specied by c(), so in this example, the vector containing the possible values of $\theta$ would be c(1/5,2/5,3/5,4/5). Use this binodp function to calculate the posterior distribution based on the data in question 5. The function will generate several output tables and one graph. Copy/Paste (do not manually copy) the last output table (the posterior distribution) and the graph into your assignment.

```{r}
library('Bolstad')

binodp(x=9, n=15, pi=c(1/5, 2/5, 3/5, 4/5), pi.prior=c(1/4, 1/4, 1/4, 1/4))

```


\begin{flushright}
  $\blacksquare$
\end{flushright}

