---
title: "STAT 8700 Homework 2"
author: "Brian Detweiler"
date: "Friday, September 9, 2016"
header-includes:
  - \usepackage[usenames, dvipsnames]{color}
output: pdf_document
---
# 1. Suppose we observe $y$ successes in $n$ trials where the probability of success in each trial is $\theta$ and suppose we use a $Beta(1, 1)$ prior for $\theta$. Show that the posterior mean can be  written as a weighted average  of the prior mean and the observed proportion of successes, meaning that the posterior mean will always fall between those two values.

Recognizing that $Beta(1, 1)$ is the uniform distribution, $U(0, 1)$. Since a uniform distribution must integrate to 1, then $\int p(\theta) d \theta = 1$, we have

$$
\begin{split}
  p(\theta | y) &\propto p(y | \theta) p(\theta) \\
  &= \binom{n}{y} \theta^y (1 - \theta)^{n - y} \cdot 1\\
  &\propto \theta^y (1-\theta)^{n - y}, 0 \leq \theta \leq 1\\
  &= c \cdot \theta^y (1 - \theta)^{n - y}\\
\end{split}
$$

We need to find the missing constant, $c$. Recognizing that the functional form of $p(\theta | y)$ is the same as $Beta(\theta | \alpha + y, \beta + n - y)$, then we have conjugacy. We now know the expected value of $Beta(\theta | \alpha + y, \beta + n - y)$ which turns out to be

$$
\begin{split}
  E[\theta | y] &= \frac{\alpha + y}{\alpha + y + \beta + n - y}\\
  &= \frac{\alpha + y}{\alpha + \beta + n}\\
\end{split}
$$

Since we have the uniform distribution, which is a special case of the Beta, $Beta(1, 1)$, then $\alpha = 1, \beta = 1$, and thus we have

$$
\begin{split}
  E[\theta | y] &= \frac{y + 1}{y + 1 + n - y + 1}\\
  &= \frac{y + 1}{n + 2}\\
  &= \frac{n}{n} \frac{y}{n + 2} + \frac{2}{2}\frac{1}{n + 2}\\
  &= \frac{n}{n + 2} \frac{y}{n} + \frac{2}{n + 2} \frac{1}{2}\\
\end{split}
$$

Because $\frac{n}{n + 2} + \frac{2}{n + 2}$ sum to 1, it should be clear that this is a weighted average of $\frac{y}{n} + \frac{1}{2}$, where $\frac{y}{n}$ is the proportion of successes, and $\frac{1}{2}$ is the prior (uniform) mean. 

Thus, if we let $y$ take on extreme values, $y = 0$ and $y = n$, then we have

$$
\begin{split}
  E[\theta | y = 0] &= \frac{n}{n + 2} \frac{0}{n} + \frac{2}{n + 2} \frac{1}{2}\\
  &= \frac{1}{n + 2} \\
  E[\theta | y = n] &= \frac{n}{n + 2} \frac{n}{n} + \frac{2}{n + 2} \frac{1}{2}\\
  &= \frac{n}{n + 2} + \frac{1}{n + 2}\\
  &= \frac{n + 1}{n + 2} \\
\end{split}
$$

\begin{flushright}
  $\blacksquare$
\end{flushright}

\pagebreak

# 2. Suppose we observe $y$ successes in $n$ trials where the probability of success in each trial is $\theta$.

## (a) Prove that if we choose a $Beta(1, 1)$ (Uniform) prior then the posterior variance will be smaller than the prior variance.

Continuing from problem 1, we wish to find the posterior variance, $Var(\theta | y)$. We know the prior variance, which is the variance of the uniform, $Var(\theta) = \frac{1}{12}$.

We will show that $Var(\theta | y) < \frac{1}{12}$.

$$
\begin{split}
  Var(\theta | y) &= \frac{(\alpha + y) (\beta + n - y)}{(\alpha + \beta + n)^2 (\alpha + \beta + n + 1)}\\
  &= \frac{E[\theta | y] \big[1 - E[\theta | y]\big]}{\alpha + \beta + n + 1}\\
  &= \frac{E[\theta | y] \big[1 - E[\theta | y]\big]}{n + 3}\\
\end{split}
$$

Evaluated at the extremes of $y = 0$ and $y = n$, we have

$$
\begin{split}
  Var(\theta | y = 0) &= \frac{\frac{1}{n + 2} \big[1 - \frac{1}{n + 2} \big]}{n + 3}\\
  Var(\theta | y = n) &= \frac{\frac{n + 1}{n + 2} \big[1 - \frac{n + 1}{n + 2} \big]}{n + 3}\\
\end{split}
$$

Evaluating $n$ at the extremes of a single sample, $n = 1$ and extremely large samples, $n \rightarrow \infty$, we find

$$
\begin{split}
  Var(\theta | y = 0; n = 1) &= \frac{\frac{1}{3} \big[1 - \frac{1}{3} \big]}{4}\\
  &= \frac{1}{18}\\
  Var(\theta | y = n; n = 1) &= \frac{\frac{2}{3} \big[1 - \frac{2}{3} \big]}{4}\\
  &= \frac{1}{18}\\
  \lim_{n \rightarrow \infty} Var(\theta | y = 0) &= \frac{\frac{1}{n + 2} \big[1 - \frac{1}{n + 2} \big]}{n + 3} = 0\\
  \lim_{n \rightarrow \infty} Var(\theta | y = n) &= \frac{\frac{n + 1}{n + 2} \big[1 - \frac{n + 1}{n + 2} \big]}{n + 3} = 0\\
\end{split}
$$

So $Var(\theta | y) < Var(\theta)$ for any value of $y$, under any sample size.

\begin{flushright}
  $\blacksquare$
\end{flushright}


## (b) Show that the above isn't necessarily the case if we choose a general $Beta(\alpha, \beta)$ prior. That is, find set of values for $\alpha, \beta, n, y$ where the above is not true.

When trying to find a proof by example, it is helpful to start at the extremes. We can see this with $Beta(2, 1)$ by letting $\alpha = 2, \beta = 1, n = 1, y = 0$. Immediately, we find a point where this is the case.

$$
\begin{split}
  E[\theta | y] &= \frac{\alpha + y}{\alpha + y + \beta + n - y}\\
  &= \frac{2 + y}{3 + n}\\
\end{split}
$$

$$
\begin{split}
  Var(\theta | y) &= \frac{(\alpha + y) (\beta + n - y)}{(\alpha + \beta + n)^2 (\alpha + \beta + n + 1)}\\
  &= \frac{E[\theta | y] \big[1 - E[\theta | y]\big]}{\alpha + \beta + n + 1}\\
  &= \frac{\frac{2}{4} \big[1 - \frac{2}{4}\big]}{4}\\
  &= \frac{\frac{1}{2} \big[\frac{1}{2} \big]}{4}\\
  &= \frac{1}{16} < \frac{1}{12} \\
\end{split}
$$

\begin{flushright}
  $\blacksquare$
\end{flushright}

\pagebreak

# 3. Suppose we wish to estimate the proportion of a voting population that support a particular ballot initiative.   We choose  to use a Uniform prior for the proportion of voters who support the initiative. A random sample of 100 voters is polled and 55 are in favor of the ballot initiative. 

## (a)  Find the posterior distribution of $\theta$. 


## (b) What is the posterior mean and variance?

## (c) The \texttt{binobp} command in the \texttt{Bolstad} package in \textsf{R} will calculate the posterior for  binomial data and a beta prior. It requires 4 inputs (in order): $y, n, \alpha, \beta$. The output includes a graph of the prior and posterior distributions. Include this graph in your assignment.

## (d) The command \texttt{abline(v=location, col="colour")} adds a vertical line to a plot, where location should be replaced by the x co-ordinate of the vertical line, and colour should be replaced by the actual color. Add 3 vertical lines to your plot from the previous part: a \textbf{black} line representing the observed proportion of voters who support the initiative, a \textcolor{red}{red} line representing the prior mean, and a \textcolor{blue}{blue} line representing the posterior mean.

## (e) Also included in the output from \texttt{binobp} is a table of posterior quantiles. A $95\%$ credible interval for the posterior distribution can be found by using the $0.025$ and $0.975$ quantiles. What is this $95\%$ credible interval for your posterior distribution? (Note this interval is exactly what people wrongly assume the classical confidence interval is, that is there is a $95\%$ chance that $\theta$ will take  a value inside this interval).

## (f) What is of interest to us is whether or not the initiative will pass (that is, receive a majority of Yes votes).  The \textsf{R} command \texttt{pbeta} computes the CDF of a beta distribution and requires 3 inputs (in order): The value where you wish to evaluate the CDF, $\alpha, \beta$. Use this to calculate our posterior probability that the initiate will pass. 

\pagebreak

# 4. Consider the previous question. Suppose we wish to use an informative prior instead. We would like to use a Beta prior will a mean of 0.4 and a prior standard deviation of 0.1. What are the corresponding hyper-parameters of the prior  distribution? Repeat all the steps of the previous question, using the new prior distribution.

\pagebreak

# 5. Each city bus in Omaha is numbered. Suppose that they are numbered sequentially $1, 2, \hdots, M$.

## (a) If $M$ were known, and $Y$ represents the number of the next bus you see, find an expression for $P(Y = y | M)$.  For what values of $y$ is this valid?

## (b) Now suppose that $M$ is unknown, as we assume a geometric prior distribution on $M$, that is that 

$$
\begin{split}
  p(M) &= \frac{1}{150} \bigg( \frac{149}{150} \bigg)^{M-1} \text{ for } M = 1, 2, \hdots
\end{split}
$$

## Furthermore, suppose we observe a single bus, numbered 200. Find the posterior distribution of $M$ (up to a constant of proportionality).

## (c) Use software (for example Wolfram-Alpha or Maple) to  find the constant of proportionality for the posterior, and thus  find the posterior mean and variance.

##(d) If we had decided to use the improper uniform prior $p(M) \propto 1$, would this have produced a proper or improper posterior distribution? Show your work.

\begin{flushright}
  $\blacksquare$
\end{flushright}
